# Градиентный спуск

## Содержание
* [Основные понятия](#Основные-понятия)
* [Идея метода](#Идея-метода) 
* [Градиент](#Градиент)
* [Шаг градиентного спуска](#Шаг-градиентного-спуска)
* [Метод градиентного спуска для линейной регрессии](#Метод-градиентного-спуска-для-линейной-регрессии)
    * [Пример для линейной регрессии](#Пример-для-линейной-регрессии)
* [Метод стохастического градиентного спуска](#Метод-стохастического-градиентного-спуска)
    * [Плюсы](#Плюсы)
    * [Минусы](#Минусы)
* [Критерии останова градиентного спуска](#Критерии-останова-градиентного-спуска)

## Основные понятия

Метод градиентного спуска — метод нахождения 
локального минимума функции

> Локальный экстремум функции — общее название 
> для локального максимума и минимума  


## Идея метода 
Идти в направлении наискорейшего убывания (антиградиент)

## Градиент 
1. Градиент — направление скорейшего увеличения функции 
2. Градиент — вектор, каждая координата которого 
   представляет собой производную функции по этой координате
   
![img.png](../images/img.png)

## Шаг градиентного спуска 
Величина приближения значения к истинному. 
Чтобы попасть в следующую точку, нужно из существующей точки (шага) 
вычесть градиент текущего шага умноженного 
на коэффициент гамма (величина шага)

![img_1.png](../images/img_1.png)

## Метод градиентного спуска для линейной регрессии 
Для вектора весов `w`

![img_2.png](../images/img_2.png)

И для функции потерь `Q`

![img_3.png](../images/img_3.png)

Шаг градиентного спуска будет соответствовать:

![img_4.png](../images/img_4.png)

### Пример для линейной регрессии 
Рассмотрим функцию `a(x)`:

![img_5.png](../images/img_5.png)

Для нее функция риска будет выглядеть следующим образом: 

![img_6.png](../images/img_6.png)

Отсюда шаг градиентного спуска: 

![img_7.png](../images/img_7.png)

## Метод стохастического градиентного спуска 
Метод градиентного спуска, при котором 
на каждом шаге из выборки выбирается подвыборка размером `l`

### Плюсы 
* Ускоряет обучение
* Позволяет учиться на больших данных
* Можно обучаться на потоке данных, 
  то есть когда в выборку 
  в реальном времени поступают новые значения
  
### Минусы 
* Может не сходиться или сходиться медленно 
* Решение может быть неустойчивым 

## Критерии останова градиентного спуска 
* Очень маленькое изменение параметров:

   ![img_8.png](../images/img_8.png)
* Незначительное изменение лосса
* Достижение заданного количества шагов 
